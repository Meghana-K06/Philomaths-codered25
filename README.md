# Virtual & Audio Accessibility Platform

## Overview

This platform is designed to provide an accessible environment for specially-abled students. It includes various features like Image Captioning, Gesture-Controlled Navigation, Speech-to-Text, and Sign Language Recognition. The platform is equipped with speech synthesis for interaction and hand gesture detection for enhanced usability.

## Features

### Login/Signup Interface
- Allows students to sign up and log in using their Student ID.
- Speech prompts guide the user through each step.

### Dashboard
- Includes options like Image to Text, Sign Language, Progress Tracker, and more.
- Accessible through voice commands, such as "Press 1 for Image to Text."

### Image to Text
- Converts images to descriptive text using the BLIP Model.
- Uses Gradio to create a user-friendly interface for real-time captioning.
![Image processing](C:\Users\megha\OneDrive\Desktop\code red\WOMAN HOLDING A BOTTLE.jpg)
![Image processing](C:\Users\megha\OneDrive\Desktop\code red\text.jpg)

### Gesture Control
- **Hand Gesture Recognition** using MediaPipe and OpenCV.
  - Simulates a virtual mouse with hand gestures: index finger for movement and thumb for clicking.
  - Movements are translated to screen coordinates, and clicking is triggered by the distance between the index and thumb fingers.
![Key pointing Modeling](C:\Users\megha\OneDrive\Desktop\code red\hand.jpg)

### Accessibility Features
- Voice-based instructions and controls.
- User-friendly design to ensure smooth interaction and navigation for students with various abilities.

## Technologies Used
- **Python**: Main programming language.
- **Flask**: Web framework for backend and routing.
- **Gradio**: For the interactive BLIP Image Captioning interface.
- **MediaPipe**: For detecting hand gestures and processing them.
- **OpenCV**: For image processing and webcam interaction.
- **pyautogui**: For controlling the mouse using hand gestures.

## How It Works

### Sign Up/Login
1. Press 1: Log in by entering your Student ID.
2. Press 2: Sign up with your Name and Student ID.
3. Once logged in, the student is directed to the Dashboard.

### Dashboard Navigation
- Navigate through options like Image to Text, Sign Language, etc., by typing a choice or using voice commands.
- A virtual mouse is controlled via the webcam and hand gestures:
  - **Index Finger**: Controls the cursor movement.
  - **Thumb**: Triggers a click if near the index finger (within a 100-pixel range).

### Image Captioning
- Upload an image and get a real-time caption generated by the BLIP model.

### Sign Language 
- Converts sign language into alphabets and text
- Easy for deaf and dumb student teacher interaction
![Sign Lang](C:\Users\megha\OneDrive\Desktop\code red\sign.jpg)

### Gesture Control
- The system uses MediaPipe to detect hand positions (index and thumb fingers).
- It maps the hand position to screen coordinates, allowing for mouse movements and clicks through pyautogui.

## Instructions

![WEBSITE](C:\Users\megha\OneDrive\Desktop\code red\website.jpg)

### Sign Up/Login Process
1. Press 1: Login by entering your Student ID.
2. Press 2: Sign up with your Name and Student ID.
3. Once logged in, the student is directed to the Dashboard.

### Dashboard:
- **Image to Text**: Type 1 to enter the Image-to-Text feature.
- **Sign Language**: Type 2 to explore sign language features.
- Voice Interaction: Instructions will be spoken, guiding the user to the next action.

### Hand Gesture Control:
- **Index Finger**: Controls the mouse movement.
- **Thumb**: Triggers a click if near the index finger (within a 100-pixel range).

## Installation and Requirements

### Prerequisites
- Install Python 3.x.

### Install Dependencies
```bash
pip install opencv-python mediapipe pyautogui gradio flask transformers
